'use client'

import { useRouter } from 'next/navigation'
import { useRef, useState } from 'react'

export default function RecordingPage() {
  const router = useRouter()
  const [isListening, setIsListening] = useState(false)
  const [transcript, setTranscript] = useState('')
  const recognitionRef = useRef(null)
  const animationRef = useRef(null)
  const [volume, setVolume] = useState(0)

  const startSpeechRecognition = async () => {
    if (!('webkitSpeechRecognition' in window) || !navigator.mediaDevices) {
      alert('ì´ ë¸Œë¼ìš°ì €ëŠ” ìŒì„± ì¸ì‹ ë˜ëŠ” ë§ˆì´í¬ ì ‘ê·¼ì„ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.')
      return
    }

    const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
    const audioCtx = new (window.AudioContext || window.webkitAudioContext)()
    const source = audioCtx.createMediaStreamSource(stream)
    const analyser = audioCtx.createAnalyser()
    analyser.fftSize = 256
    const dataArray = new Uint8Array(analyser.frequencyBinCount)
    source.connect(analyser)

    const animate = () => {
      analyser.getByteFrequencyData(dataArray)
      const avg = dataArray.reduce((a, b) => a + b, 0) / dataArray.length
      setVolume(avg)
      animationRef.current = requestAnimationFrame(animate)
    }
    animate()

    const recognition = new window.webkitSpeechRecognition()
    recognition.lang = 'ko-KR'
    recognition.interimResults = false
    recognition.maxAlternatives = 1

    recognition.onresult = async (event) => {
      console.log('%cğŸ§ª onresult ë°œìƒ', 'color:blue; font-weight:bold')
      console.log('ğŸ§ª event.results:', event.results)
      console.log('ğŸ§ª mapëœ transcript:', Array.from(event.results).map(r => r[0]?.transcript))

      let result = ''
      try {
        const transcripts = Array.from(event.results)
          .map(r => r[0]?.transcript || '')
          .filter(Boolean)

        result = transcripts.join(' ').trim()

        console.log('ğŸ§¾ ì¶”ì¶œëœ transcripts ë°°ì—´:', transcripts)
        console.log('ğŸ§¾ ìµœì¢… result:', result)
      } catch (err) {
        console.error('âŒ ìŒì„± ê²°ê³¼ íŒŒì‹± ì˜¤ë¥˜:', err)
      }

      if (!result) {
        console.warn('â— ì¸ì‹ëœ ë¬¸ì¥ì´ ì—†ìŒ â†’ ë¶„ì„ ìƒëµ')
        cancelAnimationFrame(animationRef.current)
        return
      }

      setTranscript(result)
      cancelAnimationFrame(animationRef.current)
      await analyzeEmotionWithChatGPT(result)
    }

    recognition.onerror = (event) => {
      console.error('ìŒì„± ì¸ì‹ ì˜¤ë¥˜:', event.error)
    }

    recognitionRef.current = recognition
    recognition.start()
    setIsListening(true)
  }

  const analyzeEmotionWithChatGPT = async (text) => {
    if (!text || typeof text !== 'string' || text.trim() === '') {
      console.warn('â— ê°ì • ë¶„ì„ ìƒëµ: ë¹„ì–´ìˆëŠ” ì…ë ¥')
      return
    }

    console.log('%cğŸ¤ ë¶„ì„ ìš”ì²­í•  í…ìŠ¤íŠ¸:', 'color:green; font-weight:bold', text)

    try {
      const res = await fetch('/api/chatgpt-analyze', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ transcript: text }),
      })

      const data = await res.json()
      console.log('%cğŸ§  API ì‘ë‹µ:', 'color:purple; font-weight:bold', data)

      const emotion = data.emotion?.trim()
      console.log('ê°ì • ë¶„ì„ ê²°ê³¼:', emotion)

      if (emotion) {
        router.push(`/response?emotion=${encodeURIComponent(emotion)}`)
      } else {
        console.warn('â— ê°ì • ë¶„ì„ ì‹¤íŒ¨: emotion ì—†ìŒ')
      }
    } catch (err) {
      console.error('âŒ ê°ì • ë¶„ì„ ìš”ì²­ ì‹¤íŒ¨:', err)
    }
  }

  const micSize = 128 + volume * 0.8

  return (
    <div className="h-screen flex flex-col items-center justify-center px-6 text-center bg-gradient-to-br from-indigo-100 to-purple-200">
      <p className="mb-4 text-xl font-semibold text-gray-800">
        {isListening ? 'ë“£ê³  ìˆì–´ìš”... ë§í•´ë³´ì„¸ìš” ğŸ§' : 'ê¸°ë¶„ì„ ë§ë¡œ í‘œí˜„í•´ë³´ì„¸ìš”'}
      </p>

      <div className="relative flex items-center justify-center">
        {isListening && (
          <div
            className="absolute rounded-full bg-green-300 opacity-60 blur-md animate-pulse"
            style={{
              width: `${micSize}px`,
              height: `${micSize}px`,
              transition: 'all 0.1s ease-out',
            }}
          />
        )}

        <button
          onClick={startSpeechRecognition}
          disabled={isListening}
          className="relative w-32 h-32 rounded-full bg-black flex items-center justify-center text-white text-4xl shadow-lg"
        >
          ğŸ™ï¸
        </button>
      </div>

      {transcript && (
        <p className="mt-4 text-sm text-gray-600">"{transcript}"</p>
      )}
    </div>
  )
}
